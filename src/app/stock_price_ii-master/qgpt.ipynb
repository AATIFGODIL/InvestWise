import pandas as pd    
import numpy as np  
import re  
import tensorflow as tf    
from transformers import AutoTokenizer, TFAutoModelForPreTraining    
import tensorflow_model_optimization as tfmot    
from tensorflow.keras.layers import Embedding, LayerNormalization, Dense, Dropout    
from tensorflow.keras.optimizers import Adam    
from tensorflow.keras.preprocessing.sequence import pad_sequences  
from sklearn.model_selection import train_test_split  
from sklearn.preprocessing import LabelEncoder  
from tensorflow.keras.utils import to_categorical  
  
# Load a pre-trained model and tokenizer    
tokenizer = AutoTokenizer.from_pretrained("gpt2") 
base_model = TFAutoModelForPreTraining.from_pretrained("gpt2")
  
# Fine-tune the model on financial data  
  
# Data loading and preprocessing    
data = pd.read_csv("financial_data.csv")    
text_data = data['text']    
labels = data['label']  # Assuming you have a 'label' column for supervised learning  
  
# Text cleaning function  
def clean_text(text):  
    # Remove punctuations and numbers  
    text = re.sub('[^a-zA-Z]', ' ', text)  
    # Single character removal  
    text = re.sub(r"\s+[a-zA-Z]\s+", ' ', text)  
    # Removing multiple spaces  
    text = re.sub(r'\s+', ' ', text)  
    return text   
    #pass  
  
cleaned_text = [clean_text(text) for text in text_data]  
  
# Tokenization    
input_ids = [tokenizer.encode(text) for text in cleaned_text]  
input_ids = pad_sequences(input_ids, maxlen=512, dtype="long", value=0, truncating="post", padding="post")  
  
# Create attention masks  
attention_masks = []  
for seq in input_ids:  
    seq_mask = [float(i != 0) for i in seq]  
    attention_masks.append(seq_mask)  
  
# Use train_test_split to split our data into train and validation sets  
train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)  
train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=42, test_size=0.1)  
  
# Convert all data into torch tensors  
train_inputs = tf.convert_to_tensor(train_inputs)  
validation_inputs = tf.convert_to_tensor(validation_inputs)  
train_labels = tf.convert_to_tensor(train_labels)  
validation_labels = tf.convert_to_tensor(validation_labels)  
train_masks = tf.convert_to_tensor(train_masks)  
validation_masks = tf.convert_to_tensor(validation_masks)  
  
# Create an iterator of our data with torch DataLoader   
train_data = tf.data.Dataset.from_tensor_slices((train_inputs, train_masks, train_labels))  
train_sampler = tf.data.RandomSampler(train_data)  
train_dataloader = tf.data.DataLoader(train_data, sampler=train_sampler, batch_size=32)  
  
validation_data = tf.data.Dataset.from_tensor_slices((validation_inputs, validation_masks, validation_labels))  
validation_sampler = tf.data.SequentialSampler(validation_data)  
validation_dataloader = tf.data.DataLoader(validation_data, sampler=validation_sampler, batch_size=32)  
  
# Fine-tune the model  
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)  
base_model.compile(optimizer=optimizer, loss=base_model.compute_loss)  
base_model.fit(train_dataloader, epochs=3, validation_data=validation_dataloader)  
  
# Quantize the model  
quantize_model = tfmot.quantization.keras.quantize_model  
quantized_model = quantize_model(base_model)  
  
# Save the quantized model  
quantized_model.save('quantized_model.h5')  
  
# Bias detection (basic)
def check_bias(generated_text):  
    # List of biased words, this is just an example and should be extended  
    gender_biased_words = ['he', 'she', 'his', 'hers']  
    racial_biased_words = ['racial_term1', 'racial_term2']  # replace with actual racial terms  
  
    # Tokenize the generated text  
    tokens = generated_text.split()  
  
    # Check for gender bias  
    gender_bias = any(word in tokens for word in gender_biased_words)  
    if gender_bias:  
        print("Gender bias detected in the generated text.")  
  
    # Check for racial bias  
    racial_bias = any(word in tokens for word in racial_biased_words)  
    if racial_bias:  
        print("Racial bias detected in the generated text.")  
  
# Simplified attention layer  
class MultiHeadAttention(tf.keras.layers.Layer):  
    def __init__(self, num_heads, d_model, **kwargs):  
        super(MultiHeadAttention, self).__init__(**kwargs)  
        self.num_heads = num_heads  
        self.d_model = d_model  
        assert d_model % self.num_heads == 0  
        self.depth = d_model // self.num_heads  
        self.wq = Dense(d_model)  
        self.wk = Dense(d_model)  
        self.wv = Dense(d_model)  
        self.dense = Dense(d_model)  
  
    def split_heads(self, x, batch_size):  
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))  
        return tf.transpose(x, perm=[0, 2, 1, 3])  
		
	def scaled_dot_product_attention(q, k, v, mask):  
		matmul_qk = tf.matmul(q, k, transpose_b=True)  
		dk = tf.cast(tf.shape(k)[-1], tf.float32)  
		scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  
		if mask is not None:  
			scaled_attention_logits += (mask * -1e9)  
		attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  
		output = tf.matmul(attention_weights, v)  
		return output, attention_weights  

  
    def call(self, v, k, q, mask):  
        batch_size = tf.shape(q)[0]  
        q = self.wq(q)  
        k = self.wk(k)  
        v = self.wv(v)  
        q = self.split_heads(q, batch_size)  
        k = self.split_heads(k, batch_size)  
        v = self.split_heads(v, batch_size)  
        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)  
        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  
        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  
        output = self.dense(concat_attention)  
        return output, attention_weights    
  
# Training  
model = ...  # Define your model  
model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())  
model.fit(X_train, y_train, epochs=10, batch_size=32)  
  
# Example of rule-based constraints  
def apply_quantitative_constraints(generated_text):  
    # For example, let's limit the length of the generated text  
    max_length = 100  
    if len(generated_text) > max_length:  
        return generated_text[:max_length]  
    else:  
        return generated_text  
